inputs:
  transactions:
    kafka:
      config: C:\\Users\\Kapil.Sreedharan\\.confluent\\java.config
      topic: transactions
      consumerGroup: kafkaAppConsumerGroup
      options:
        startingOffsets: earliest
        failOnDataLoss: true
      schemaRegistryUrl: https://psrc-lgy7n.europe-west3.gcp.confluent.cloud

transformations:
  - src/test/resources/config/kafka_transformations.yaml

output:
  file:
    dir: target/test-classes/output/transactions/

# If set to true, triggers Explain before saving
explain: truesimpletopic

# Set Log Level : ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
logLevel: WARN

# Set Application Name to have app name prefix in spark instrumentation counters
appName: KafkaApp

streaming:
  # Set the trigger mode (ProcessingTime, Once, Continuous)
  triggerMode: Once
  # If trigger is ProcessingTime/Continuous set the trigger duration
  triggerDuration: 10 seconds
  # Possible values are append/replace/complete
  outputMode: append
  # Where to save Spark's checkpoint
  checkpointLocation: target/test-classes/output/checkpoint
  # Optionally set streaming to use foreachBatch when writing streams. this enable writing to all available writers and to write to multiple outputs.
  batchMode: true
  format: parquet
  path: target/test-classes/output/transactions/
  # Add any other options supported by the DataStreamWriter
  extraOptions:
    opt: val
