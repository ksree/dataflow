# dataflow : A  framework that lets you easily create spark ETL jobs  using simple configuration files

Dataflow is a software paradigm based on the idea of disconnecting computational actors into stages (pipelines) that can execute concurrently

Set the GCP credentials in the env. variable:
GOOGLE_APPLICATION_CREDENTIALS=C:\Users\Kapil.Sreedharan\secrets\dataflow\blade-ai-282114-34167c2579bd.json
 
Create a temporary bucket 
gsutil mb gs://tmpdataflowbucketkafka

IAC: 

Usefull links:
 https://cloud.google.com/solutions/managing-infrastructure-as-code
https://github.com/antoniocachuan/IaC-boilerplate
https://medium.com/@routdeepak/writing-to-aws-s3-from-spark-91e85d09724b